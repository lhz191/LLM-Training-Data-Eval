#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Static Executability æŒ‡æ ‡ - GUI Agent é™æ€å¯æ‰§è¡Œæ€§æ£€æŸ¥

é€šç”¨æ¡†æ¶ï¼Œæ”¯æŒä¸åŒæ•°æ®é›†çš„é™æ€å¯æ‰§è¡Œæ€§æ£€æŸ¥ï¼š
- é€šç”¨æŒ‡æ ‡ï¼štotal_records, total_actions, errors, warnings
- æ•°æ®é›†ç‰¹æœ‰æŒ‡æ ‡ï¼šç”±å„ checker è¿”å›ï¼Œè‡ªåŠ¨åˆå¹¶åˆ°ç»“æœä¸­

ä½¿ç”¨æ–¹å¼:
    from metrics import compute_static_executability
    from loaders import Mind2WebLoader
    from mind2web_executor import Mind2WebStaticChecker
    
    loader = Mind2WebLoader('/path/to/mind2web/data')
    checker = Mind2WebStaticChecker(raw_dump_path='/path/to/raw_dump')
    
    results = compute_static_executability(
        data_iterator=loader.iterate(),
        static_checker=checker,
        dataset_name='Mind2Web',
        output_file='static_executability_results.json'
    )
"""

import json
import time
from datetime import datetime
from typing import Optional, Iterator, Dict, List, Any

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data_types import Record
from text_gui_executor import StaticExecutabilityChecker


def compute_static_executability(
    data_iterator: Iterator[Record],
    static_checker: StaticExecutabilityChecker,
    dataset_name: str = "unknown",
    output_file: Optional[str] = None,
    max_samples: Optional[int] = None,
    progress_interval: int = 10,
) -> Dict[str, Any]:
    """
    è®¡ç®—é™æ€å¯æ‰§è¡Œæ€§æŒ‡æ ‡
    
    Args:
        data_iterator: Record è¿­ä»£å™¨
        static_checker: é™æ€å¯æ‰§è¡Œæ€§æ£€æŸ¥å™¨
        dataset_name: æ•°æ®é›†åç§°
        output_file: ç»“æœè¾“å‡ºæ–‡ä»¶
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        progress_interval: è¿›åº¦æ˜¾ç¤ºé—´éš”
    
    Returns:
        ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
        - é€šç”¨æŒ‡æ ‡ï¼štotal_records, total_actions, errors, warnings
        - æ•°æ®é›†ç‰¹æœ‰æŒ‡æ ‡ï¼šç”± checker è¿”å›ï¼Œè‡ªåŠ¨ç´¯åŠ 
    """
    print("=" * 70)
    print("Static Executability Evaluation")
    print("=" * 70)
    print(f"æ•°æ®é›†: {dataset_name}")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    start_time = time.time()
    
    # =========================================================================
    # é€šç”¨ç»Ÿè®¡ï¼ˆæ‰€æœ‰æ•°æ®é›†å…±æœ‰ï¼‰
    # =========================================================================
    total_records = 0
    records_with_errors = 0
    records_with_warnings = 0
    total_actions = 0
    
    # =========================================================================
    # æ•°æ®é›†ç‰¹æœ‰ç»Ÿè®¡ï¼ˆä» checker è¿”å›çš„ stats ä¸­ç´¯åŠ æ•°å€¼ç±»å‹å­—æ®µï¼‰
    # =========================================================================
    dataset_specific_stats = {}  # ç´¯åŠ çš„æ•°å€¼ç»Ÿè®¡
    
    # è¯¦ç»†ç»“æœ
    record_results = []
    error_records = []
    
    for record in data_iterator:
        if max_samples and total_records >= max_samples:
            break
        
        total_records += 1
        
        # æ£€æŸ¥å¯æ‰§è¡Œæ€§ - checker è¿”å› (errors, warnings, stats)
        errors, warnings, stats = static_checker.check(record)
        
        # Record çº§åˆ«ç»Ÿè®¡
        if errors:
            records_with_errors += 1
            error_records.append({
                'sample_id': record.sample_id,
                'annotation_id': record.metadata.get('annotation_id', ''),
                'errors': errors,
            })
        if warnings:
            records_with_warnings += 1
        
        # é€šç”¨ Action ç»Ÿè®¡
        total_actions += stats.get('total_actions', 0)
        
        # ç´¯åŠ æ•°æ®é›†ç‰¹æœ‰çš„æ•°å€¼ç»Ÿè®¡ï¼ˆè·³è¿‡ rate ç±»å‹çš„å­—æ®µï¼Œè¿™äº›éœ€è¦æœ€åé‡æ–°è®¡ç®—ï¼‰
        for key, value in stats.items():
            if isinstance(value, (int, float)) and key != 'total_actions':
                # è·³è¿‡æ¯”ç‡å­—æ®µï¼ˆåŒ…å« rate çš„å­—æ®µä¸åº”è¯¥ç´¯åŠ ï¼‰
                if 'rate' in key.lower():
                    continue
                if key not in dataset_specific_stats:
                    dataset_specific_stats[key] = 0
                dataset_specific_stats[key] += value
        
        # è®°å½•è¯¦ç»†ç»“æœï¼ˆä¿ç•™ checker è¿”å›çš„æ‰€æœ‰ statsï¼ŒåŒ…æ‹¬ action_resultsï¼‰
        record_result = {
            'sample_id': record.sample_id,
            'annotation_id': record.metadata.get('annotation_id', ''),
            'website': record.website,
            'errors': errors,
            'warnings': warnings,
        }
        # åˆå¹¶ stats ä¸­çš„æ‰€æœ‰å­—æ®µï¼ˆåŒ…æ‹¬ action_resultsï¼‰
        for key, value in stats.items():
            record_result[key] = value
        record_results.append(record_result)
        
        # è¿›åº¦
        if progress_interval and total_records % progress_interval == 0:
            elapsed = time.time() - start_time
            rate = total_records / elapsed if elapsed > 0 else 0
            print(f"  [{total_records:,}] {rate:.2f} æ¡/ç§’")
    
    elapsed = time.time() - start_time
    
    # =========================================================================
    # æ„å»ºç»“æœ
    # =========================================================================
    results = {
        'dataset': dataset_name,
        'timestamp': datetime.now().isoformat(),
        'elapsed_seconds': elapsed,
        
        # é€šç”¨ç»Ÿè®¡
        'total_records': total_records,
        'records_with_errors': records_with_errors,
        'records_with_warnings': records_with_warnings,
        'total_actions': total_actions,
        
        # è¯¦ç»†ç»“æœ
        'record_results': record_results,
        'error_records': error_records,
    }
    
    # åˆå¹¶æ•°æ®é›†ç‰¹æœ‰ç»Ÿè®¡ï¼Œå¹¶è®¡ç®—æ¯”ç‡
    # WebShop: 
    #   - success_rate = success_count / total_actions (åŠ¨ä½œæ‰§è¡ŒæˆåŠŸç‡)
    #   - task_completion_rate = task_completed / total_records (ä»»åŠ¡å®Œæˆç‡)
    #   - task_success_rate = task_success / total_records (ä»»åŠ¡æˆåŠŸç‡ï¼Œå®Œæˆä¸”reward>0)
    if 'success_count' in dataset_specific_stats and total_actions > 0:
        dataset_specific_stats['action_success_rate'] = dataset_specific_stats['success_count'] / total_actions
    if 'task_completed' in dataset_specific_stats and total_records > 0:
        dataset_specific_stats['task_completion_rate'] = dataset_specific_stats['task_completed'] / total_records
    if 'task_success' in dataset_specific_stats and total_records > 0:
        dataset_specific_stats['task_success_rate'] = dataset_specific_stats['task_success'] / total_records
    if 'task_partial' in dataset_specific_stats and total_records > 0:
        dataset_specific_stats['task_partial_rate'] = dataset_specific_stats['task_partial'] / total_records
    # final_reward æ”¹ä¸ºå¹³å‡å€¼
    if 'final_reward' in dataset_specific_stats and total_records > 0:
        dataset_specific_stats['avg_reward'] = dataset_specific_stats['final_reward'] / total_records
        del dataset_specific_stats['final_reward']  # åˆ é™¤ç´¯åŠ å€¼ï¼Œåªä¿ç•™å¹³å‡å€¼
    
    # Mind2Web: coord_rate, attr_rate
    verified = dataset_specific_stats.get('verified_actions', 0)
    if verified > 0:
        if 'coord_success' in dataset_specific_stats:
            dataset_specific_stats['coord_rate'] = dataset_specific_stats['coord_success'] / verified
        if 'attr_success' in dataset_specific_stats:
            dataset_specific_stats['attr_rate'] = dataset_specific_stats['attr_success'] / verified
    
    results['dataset_specific_stats'] = dataset_specific_stats
    
    # ä¿å­˜ç»“æœ
    if output_file:
        output_dir = os.path.dirname(output_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç”Ÿæˆç®€æ´çš„æ±‡æ€» log æ–‡ä»¶
        summary_file = output_file.replace('.json', '_summary.txt')
        _save_summary_log(results, summary_file, elapsed, dataset_name)
    
    # æ‰“å°æ‘˜è¦
    _print_summary(results, elapsed, dataset_name)
    
    return results


def _save_summary_log(results: Dict[str, Any], summary_file: str, elapsed: float, dataset_name: str):
    """ä¿å­˜ç®€æ´çš„æ±‡æ€» log æ–‡ä»¶"""
    lines = []
    lines.append("=" * 60)
    lines.append(f"Static Executability è¯„ä¼°æ±‡æ€» - {dataset_name}")
    lines.append("=" * 60)
    lines.append(f"æ—¶é—´: {results.get('timestamp', 'N/A')}")
    lines.append(f"è€—æ—¶: {elapsed:.1f} ç§’")
    lines.append("")
    
    lines.append("ã€Record çº§åˆ«ã€‘")
    lines.append(f"  æ€» Record æ•°: {results['total_records']:,}")
    lines.append(f"  æœ‰é”™è¯¯: {results['records_with_errors']:,}")
    lines.append(f"  æœ‰è­¦å‘Š: {results['records_with_warnings']:,}")
    lines.append("")
    
    lines.append("ã€Action çº§åˆ«ã€‘")
    lines.append(f"  æ€» Action æ•°: {results['total_actions']:,}")
    lines.append("")
    
    # æ•°æ®é›†ç‰¹æœ‰ç»Ÿè®¡
    dataset_stats = results.get('dataset_specific_stats', {})
    if dataset_stats:
        lines.append(f"ã€{dataset_name} ç‰¹æœ‰æŒ‡æ ‡ã€‘")
        for key, value in sorted(dataset_stats.items()):
            if isinstance(value, float):
                lines.append(f"  {key}: {value:.4f}")
            else:
                lines.append(f"  {key}: {value:,}")
        lines.append("")
    
    # å…³é”®æŒ‡æ ‡æ±‡æ€»ï¼ˆæ”¾åœ¨æœ€æ˜¾çœ¼çš„ä½ç½®ï¼‰
    lines.append("=" * 60)
    lines.append("ã€å…³é”®æŒ‡æ ‡æ±‡æ€»ã€‘")
    lines.append("=" * 60)
    
    if 'task_success_rate' in dataset_stats:
        lines.append(f"  âœ… ä»»åŠ¡æˆåŠŸç‡ (reward=1.0): {dataset_stats['task_success_rate']:.2%}")
    if 'task_partial_rate' in dataset_stats:
        lines.append(f"  âš ï¸ éƒ¨åˆ†æˆåŠŸç‡ (0<reward<1): {dataset_stats['task_partial_rate']:.2%}")
    if 'avg_reward' in dataset_stats:
        lines.append(f"  ğŸ“Š å¹³å‡ reward: {dataset_stats['avg_reward']:.4f}")
    if 'coord_rate' in dataset_stats:
        lines.append(f"  ğŸ“ åæ ‡å®šä½æˆåŠŸç‡: {dataset_stats['coord_rate']:.2%}")
    if 'attr_rate' in dataset_stats:
        lines.append(f"  ğŸ·ï¸ å±æ€§å®šä½æˆåŠŸç‡: {dataset_stats['attr_rate']:.2%}")
    
    lines.append("")
    lines.append("=" * 60)
    lines.append(f"è¯¦ç»†ç»“æœ: {summary_file.replace('_summary.txt', '.json')}")
    lines.append("=" * 60)
    
    # å†™å…¥æ–‡ä»¶
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    print(f"æ±‡æ€»å·²ä¿å­˜åˆ°: {summary_file}")


def _print_summary(results: Dict[str, Any], elapsed: float, dataset_name: str):
    """æ‰“å°è¯„ä¼°æ‘˜è¦"""
    print()
    print("=" * 70)
    print(f"è¯„ä¼°å®Œæˆï¼è€—æ—¶ {elapsed:.1f} ç§’")
    print("=" * 70)
    print()
    print(f"ã€Record çº§åˆ«ã€‘")
    print(f"  æ€» Record æ•°: {results['total_records']:,}")
    print(f"  æœ‰é”™è¯¯: {results['records_with_errors']:,}")
    print(f"  æœ‰è­¦å‘Š: {results['records_with_warnings']:,}")
    print()
    print(f"ã€Action çº§åˆ«ã€‘")
    print(f"  æ€» Action æ•°: {results['total_actions']:,}")
    print()
    
    # æ‰“å°æ•°æ®é›†ç‰¹æœ‰ç»Ÿè®¡
    dataset_stats = results.get('dataset_specific_stats', {})
    if dataset_stats:
        print(f"ã€{dataset_name} ç‰¹æœ‰æŒ‡æ ‡ã€‘")
        for key, value in sorted(dataset_stats.items()):
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
            else:
                print(f"  {key}: {value:,}")
    print()
    
    # æŒ‰ç½‘ç«™ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰å¤šä¸ªç½‘ç«™ï¼‰
    website_stats = {}
    for r in results['record_results']:
        site = r.get('website') or 'unknown'
        if site not in website_stats:
            website_stats[site] = {'records': 0, 'actions': 0}
        website_stats[site]['records'] += 1
        website_stats[site]['actions'] += r.get('total_actions', 0)
    
    if len(website_stats) > 1:
        print(f"ã€æŒ‰ç½‘ç«™ç»Ÿè®¡ã€‘")
        sorted_sites = sorted(website_stats.items(), key=lambda x: -x[1]['records'])
        for site, stats in sorted_sites[:10]:
            print(f"  {site}: {stats['records']} records, {stats['actions']} actions")
        if len(sorted_sites) > 10:
            print(f"  ... è¿˜æœ‰ {len(sorted_sites) - 10} ä¸ªç½‘ç«™")
        print()


# =============================================================================
# å‘½ä»¤è¡Œå…¥å£
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="é™æ€å¯æ‰§è¡Œæ€§æŒ‡æ ‡è¯„ä¼°")
    parser.add_argument("--dataset", type=str, required=True, 
                        choices=["mind2web", "webshop", "weblinx"],
                        help="æ•°æ®é›†åç§°")
    parser.add_argument("--data-path", type=str, default=None,
                        help="æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--raw-dump", type=str, default=None,
                        help="raw_dump è·¯å¾„ (Mind2Web)")
    parser.add_argument("--max-samples", type=int, default=None,
                        help="æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    parser.add_argument("--output", type=str, default=None,
                        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--show", action="store_true",
                        help="æ˜¾ç¤ºæµè§ˆå™¨çª—å£")
    parser.add_argument("--progress-interval", type=int, default=10,
                        help="è¿›åº¦æ˜¾ç¤ºé—´éš”")
    
    args = parser.parse_args()
    
    # æ ¹æ®æ•°æ®é›†é€‰æ‹© loader å’Œ checker
    if args.dataset == "mind2web":
        from loaders import Mind2WebLoader
        from mind2web_executor import Mind2WebStaticChecker
        
        data_path = args.data_path or '/mnt/petrelfs/liuhaoze/datasets/Agent_Data/Mind2Web/data'
        loader = Mind2WebLoader(data_path)
        loader.load()
        
        checker = Mind2WebStaticChecker(
            raw_dump_path=args.raw_dump,
            headless=not args.show,
        )
        dataset_name = "Mind2Web"
        
    elif args.dataset == "webshop":
        from loaders import WebShopLoader
        from webshop_executor import WebShopStaticChecker
        
        data_path = args.data_path or os.path.join(os.path.dirname(os.path.dirname(__file__)), 'webshop/baseline_models/data/il_trajs_finalized_images.jsonl')
        loader = WebShopLoader(data_path)
        
        checker = WebShopStaticChecker(
            use_browser=args.show,  # --show è¡¨ç¤ºä½¿ç”¨ browser æ¨¡å¼
            render=args.show,
        )
        dataset_name = "WebShop"
        
    elif args.dataset == "weblinx":
        # TODO: WebLINX checker è¿˜æœªå®ç°
        raise NotImplementedError("WebLINX static checker not implemented yet")
    
    # è®¾ç½®è¾“å‡ºæ–‡ä»¶
    output_file = args.output
    if output_file is None:
        output_file = f"results/{args.dataset}/static_executability_results.json"
    
    # è¿è¡Œè¯„ä¼°
    results = compute_static_executability(
        data_iterator=loader.iterate(),
        static_checker=checker,
        dataset_name=dataset_name,
        output_file=output_file,
        max_samples=args.max_samples,
        progress_interval=args.progress_interval,
    )
