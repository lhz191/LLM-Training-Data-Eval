#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°å­¦æ•°æ®é›† Validity + Faithfulness + Format Check + Diversity éªŒè¯

æ”¯æŒçš„æ•°æ®é›†:
- OpenMathInstruct-1
- LILA (Math)

æ”¯æŒçš„æŒ‡æ ‡:
- Validity: ä»£ç æ‰§è¡ŒéªŒè¯
- Faithfulness: LLM-as-Judge æ¨ç†è´¨é‡è¯„ä¼°
- Reasoning Validity: LLM-as-Judge æ¨ç†é€»è¾‘æ­£ç¡®æ€§è¯„ä¼°
- Format Check: æ•°æ®æ ¼å¼éªŒè¯
- Diversity: å¤šæ ·æ€§è¯„ä¼° (Vendi Score / KNN)
"""
import sys
sys.set_int_max_str_digits(0)

import os
import argparse
import itertools
from datetime import datetime

from validity import compute_validity, compute_validity_parallel
from faithfulness import compute_faithfulness
from reasoning_validity import compute_reasoning_validity, compute_reasoning_validity_parallel
from format_check import compute_format_check, compute_format_check_parallel
from diversity import compute_diversity
from code_executor import get_comparator
from openmath_executor import (
    OpenMathCodeExtractor, OpenMathExecutor, OpenMathExecutorFast, 
    BoxedAnswerExtractor, DirectAnswerExtractor,
    OpenMathResultComparator, OpenMathFormatChecker
)
from lila_executor import (
    LILACodeExtractor, LILACodeExecutor,
    LILAResultComparator, LILAFormatChecker
)
from loaders import OpenMathInstructLoader, LILALoader


# =============================================================================
# æ•°æ®é›†é…ç½®
# =============================================================================

DATASETS = {
    'openmathinstruct': {
        'name': 'OpenMathInstruct-1',
        'data_path': '/mnt/petrelfs/liuhaoze/datasets/Symbolic_and_Logical_Data/OpenMathInstruct-1',
        'result_file': 'results/openmath/validity_results.json',
        'log_file': 'results/openmath/validity_details.log',
        'loader_class': OpenMathInstructLoader,
        'loader_kwargs': {'use_correct': True},
        'code_extractor': OpenMathCodeExtractor,
        'executor': OpenMathExecutorFast,  # Fast æ‰§è¡Œå™¨
        'answer_extractor': BoxedAnswerExtractor,
        'comparator': OpenMathResultComparator,
        'format_checker': OpenMathFormatChecker,
        'progress_interval': 1000,
        # å¤šæ ·æ€§é…ç½®
        'diversity_method': 'knn',  # 'knn' æˆ– 'vendi'
        'diversity_sample_size': None,  # None è¡¨ç¤ºå…¨é‡ï¼Œæ•°å­—è¡¨ç¤ºé‡‡æ ·
        'embedding_cache': 'embeddings/openmath_question.npy',
        'embedding_model': 'all-MiniLM-L6-v2',  # æˆ– 'Qwen/Qwen3-Embedding-8B'
    },
    'lila': {
        'name': 'LILA-Math',
        'data_path': '/mnt/petrelfs/liuhaoze/datasets/Symbolic_and_Logical_Data/LILA/lila/multi/iid/train_math_only.json',
        'result_file': 'results/lila/validity_results.json',
        'log_file': 'results/lila/validity_details.log',
        'loader_class': LILALoader,
        'loader_kwargs': {},
        'code_extractor': LILACodeExtractor,
        'executor': LILACodeExecutor,
        'answer_extractor': DirectAnswerExtractor,
        'comparator': LILAResultComparator,
        'format_checker': LILAFormatChecker,
        'progress_interval': 10000,
        # å¤šæ ·æ€§é…ç½®
        'diversity_method': 'knn',  # 'knn' æˆ– 'vendi'
        'diversity_sample_size': None,  # None è¡¨ç¤ºå…¨é‡
        'embedding_cache': 'embeddings/lila_question.npy',
        'embedding_model': 'all-MiniLM-L6-v2',  # æˆ– 'Qwen/Qwen3-Embedding-8B'
    },
}


# =============================================================================
# æ—¥å¿—å†™å…¥
# =============================================================================

def write_detailed_log(results: dict, log_path: str, dataset_name: str):
    """å†™å…¥è¯¦ç»†çš„æ—¥å¿—æ–‡ä»¶"""
    with open(log_path, 'w', encoding='utf-8') as f:
        # Header
        f.write(f"# {dataset_name} Validity Analysis\n")
        f.write(f"# Timestamp: {results['timestamp']}\n")
        f.write(f"# Elapsed: {results['elapsed_seconds']:.1f}s\n")
        f.write("=" * 80 + "\n\n")
        
        # Statistics
        f.write("## Statistics\n\n")
        total = results['total']
        with_code = results['with_code']
        no_code = results['no_code']
        
        f.write(f"Total Samples: {total:,}\n")
        f.write(f"  With Code: {with_code:,} ({with_code/total*100:.1f}%)\n")
        f.write(f"  No Code (Pure CoT): {no_code:,} ({no_code/total*100:.1f}%)\n\n")
        
        f.write("### Code Execution Verification (samples with code)\n")
        f.write(f"  âœ… Match: {results['code_matches']:,}\n")
        f.write(f"  âŒ Exec Error: {results['code_exec_errors']:,}\n")
        f.write(f"  âŒ Mismatch: {results['code_mismatches']:,}\n")
        f.write(f"  âš ï¸  No Expected Output: {results['code_no_expected']:,}\n")
        f.write(f"  ğŸ“Š Code Acc: {results['code_acc']:.4f} ({results['code_acc']:.2%})\n\n")
        
        if no_code > 0:
            f.write("### Answer Verification (samples without code)\n")
            f.write(f"  âœ… Match: {results['nl_matches']:,}\n")
            f.write(f"  âŒ Mismatch: {results['nl_mismatches']:,}\n")
            f.write(f"  âš ï¸  No Answer Extracted: {results['nl_no_answer']:,}\n")
            f.write(f"  âš ï¸  No Ground Truth: {results['nl_no_gt']:,}\n")
            f.write(f"  ğŸ“Š NL Acc: {results['nl_acc']:.4f} ({results['nl_acc']:.2%})\n\n")
        
        f.write("### Overall Metrics\n")
        f.write(f"  ğŸ“Š Overall Acc: {results['overall_acc']:.4f} ({results['overall_acc']:.2%})\n\n")
        
        f.write("=" * 80 + "\n\n")
        
        # Code Error Samples (é™åˆ¶æ•°é‡)
        code_error_samples = results['code_error_samples']
        f.write(f"## âŒ Code Execution Error Details (Total: {len(code_error_samples)})\n\n")
        
        max_samples = 100
        for i, sample in enumerate(code_error_samples[:max_samples]):
            f.write(f"### Code Error #{i+1}\n")
            f.write(f"Sample ID: {sample['sample_id']}\n")
            f.write(f"Ground Truth: {sample['ground_truth']}\n")
            f.write(f"Error: {sample['error']}\n\n")
            question = sample['question'][:500] + '...' if len(sample['question']) > 500 else sample['question']
            f.write(f"Question:\n{question}\n\n")
            if sample['code']:
                code = sample['code'][:1000] + '...' if len(sample['code']) > 1000 else sample['code']
                f.write(f"Code:\n```python\n{code}\n```\n\n")
            f.write("-" * 80 + "\n\n")
        
        if len(code_error_samples) > max_samples:
            f.write(f"... è¿˜æœ‰ {len(code_error_samples) - max_samples} ä¸ªé”™è¯¯æ ·æœ¬æœªæ˜¾ç¤º ...\n\n")
        
        f.write("=" * 80 + "\n\n")
        
        # Code Mismatch Samples
        code_mismatch_samples = results['code_mismatch_samples']
        f.write(f"## âŒ Code Result Mismatch Details (Total: {len(code_mismatch_samples)})\n\n")
        
        for i, sample in enumerate(code_mismatch_samples[:max_samples]):
            f.write(f"### Code Mismatch #{i+1}\n")
            f.write(f"Sample ID: {sample['sample_id']}\n")
            f.write(f"Ground Truth: {sample['ground_truth']}\n\n")
            question = sample['question'][:500] + '...' if len(sample['question']) > 500 else sample['question']
            f.write(f"Question:\n{question}\n\n")
            if sample['code']:
                code = sample['code'][:1000] + '...' if len(sample['code']) > 1000 else sample['code']
                f.write(f"Code:\n```python\n{code}\n```\n\n")
            f.write(f"Expected Output:\n```\n{sample['expected']}\n```\n\n")
            f.write(f"Actual Output:\n```\n{sample['actual']}\n```\n\n")
            f.write("-" * 80 + "\n\n")
        
        if len(code_mismatch_samples) > max_samples:
            f.write(f"... è¿˜æœ‰ {len(code_mismatch_samples) - max_samples} ä¸ªä¸åŒ¹é…æ ·æœ¬æœªæ˜¾ç¤º ...\n\n")
        
        f.write("=" * 80 + "\n\n")
        
        # NL Mismatch Samples
        nl_mismatch_samples = results['nl_mismatch_samples']
        if nl_mismatch_samples:
            f.write(f"## âŒ Answer Mismatch Details (Total: {len(nl_mismatch_samples)})\n\n")
            
            for i, sample in enumerate(nl_mismatch_samples[:max_samples]):
                f.write(f"### Answer Mismatch #{i+1}\n")
                f.write(f"Sample ID: {sample['sample_id']}\n")
                f.write(f"Extracted Answer: {sample['extracted_answer']}\n")
                f.write(f"Ground Truth: {sample['ground_truth']}\n\n")
                question = sample['question'][:500] + '...' if len(sample['question']) > 500 else sample['question']
                f.write(f"Question:\n{question}\n\n")
                solution = str(sample['solution'])[:1000] + '...' if len(str(sample['solution'])) > 1000 else sample['solution']
                f.write(f"Solution:\n{solution}\n\n")
                f.write("-" * 80 + "\n\n")
            
            if len(nl_mismatch_samples) > max_samples:
                f.write(f"... è¿˜æœ‰ {len(nl_mismatch_samples) - max_samples} ä¸ªä¸åŒ¹é…æ ·æœ¬æœªæ˜¾ç¤º ...\n\n")
            
            f.write("=" * 80 + "\n\n")
        
        f.write("# End of Report\n")


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def run_dataset(dataset_key: str, parallel: bool = False, num_workers: int = None):
    """è¿è¡ŒæŒ‡å®šæ•°æ®é›†çš„éªŒè¯
    
    Args:
        dataset_key: æ•°æ®é›†æ ‡è¯†
        parallel: æ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œ
        num_workers: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆä»…å½“ parallel=True æ—¶æœ‰æ•ˆï¼‰
    """
    if dataset_key not in DATASETS:
        print(f"æœªçŸ¥æ•°æ®é›†: {dataset_key}")
        print(f"å¯ç”¨æ•°æ®é›†: {list(DATASETS.keys())}")
        return
    
    config = DATASETS[dataset_key]
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_file = os.path.join(script_dir, config['result_file'])
    log_file = os.path.join(script_dir, config['log_file'])
    
    print(f"\n{'='*70}")
    print(f"å¼€å§‹éªŒè¯: {config['name']}")
    if parallel:
        print(f"æ¨¡å¼: å¤šè¿›ç¨‹å¹¶è¡Œ (workers={num_workers or 'auto'})")
    else:
        print(f"æ¨¡å¼: å•è¿›ç¨‹ä¸²è¡Œ")
    print(f"{'='*70}\n")
    
    # åˆ›å»º loader
    loader = config['loader_class'](config['data_path'], **config['loader_kwargs'])
    
    if parallel:
        # å¤šè¿›ç¨‹å¹¶è¡Œæ¨¡å¼
        results = compute_validity_parallel(
            data_iterator=loader.iterate(),
            code_extractor_class=config['code_extractor'],
            executor_class=config['executor'],
            answer_extractor_class=config['answer_extractor'],
            comparator_class=config['comparator'],
            output_file=output_file,
            progress_interval=config['progress_interval'],
            dataset_name=config['name'],
            num_workers=num_workers,
        )
    else:
        # å•è¿›ç¨‹ä¸²è¡Œæ¨¡å¼
        results = compute_validity(
            data_iterator=loader.iterate(),
            code_extractor=config['code_extractor'](),
            executor=config['executor'](),
            answer_extractor=config['answer_extractor'](),
            comparator=config['comparator'](),
            output_file=output_file,
            progress_interval=config['progress_interval'],
            dataset_name=config['name']
    )
    
    # å†™å…¥è¯¦ç»†æ—¥å¿—
    write_detailed_log(results, log_file, config['name'])
    print(f"\nè¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
    
    # ç®€è¦æ‰“å°
    print("\n" + "=" * 70)
    print(f"ä»£ç æ‰§è¡Œé”™è¯¯æ ·æœ¬æ•°: {len(results['code_error_samples'])}")
    print(f"ä»£ç ç»“æœä¸åŒ¹é…æ ·æœ¬æ•°: {len(results['code_mismatch_samples'])}")
    if results['nl_mismatch_samples']:
        print(f"ç­”æ¡ˆä¸åŒ¹é…æ ·æœ¬æ•°: {len(results['nl_mismatch_samples'])}")
    print("=" * 70)
    
    # æ‰“å°å‰å‡ ä¸ªé”™è¯¯æ ·æœ¬
    if results['code_error_samples']:
        print("\nä»£ç æ‰§è¡Œé”™è¯¯é¢„è§ˆ (å‰ 5 ä¸ª):")
        for sample in results['code_error_samples'][:5]:
            print(f"  - {sample['sample_id']}: {sample['error'][:60]}...")
    
    if results['code_mismatch_samples']:
        print("\nä»£ç ç»“æœä¸åŒ¹é…é¢„è§ˆ (å‰ 5 ä¸ª):")
        for sample in results['code_mismatch_samples'][:5]:
            exp_preview = str(sample['expected'])[:30].replace('\n', ' ')
            act_preview = str(sample['actual'])[:30].replace('\n', ' ') if sample['actual'] else 'None'
            print(f"  - {sample['sample_id']}: expected='{exp_preview}...' actual='{act_preview}...'")
    
    if results['nl_mismatch_samples']:
        print("\nç­”æ¡ˆä¸åŒ¹é…é¢„è§ˆ (å‰ 5 ä¸ª):")
        for sample in results['nl_mismatch_samples'][:5]:
            print(f"  - {sample['sample_id']}: extracted='{sample['extracted_answer']}' gt='{sample['ground_truth']}'")
    
    return results


def run_faithfulness(dataset_key: str, max_samples: int = None):
    """è¿è¡ŒæŒ‡å®šæ•°æ®é›†çš„ Faithfulness è¯„ä¼°"""
    if dataset_key not in DATASETS:
        print(f"æœªçŸ¥æ•°æ®é›†: {dataset_key}")
        return
    
    config = DATASETS[dataset_key]
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_file = os.path.join(script_dir, f"{dataset_key}_faithfulness_results.json")
    
    print(f"\n{'='*70}")
    print(f"Faithfulness è¯„ä¼°: {config['name']}")
    if max_samples:
        print(f"æ ·æœ¬é™åˆ¶: {max_samples}")
    else:
        print(f"æ¨¡å¼: å…¨é‡")
    print(f"{'='*70}\n")
    
    # åˆ›å»º loader
    loader = config['loader_class'](config['data_path'], **config['loader_kwargs'])
    
    # å¦‚æœæœ‰æ ·æœ¬é™åˆ¶ï¼Œä½¿ç”¨ islice
    if max_samples:
        data_iter = itertools.islice(loader.iterate(), max_samples)
    else:
        data_iter = loader.iterate()
    
    # è¿è¡Œè¯„ä¼°
    results = compute_faithfulness(
        data_iterator=data_iter,
        output_file=output_file,
        progress_interval=1000,  # å…¨é‡æ—¶æ¯ 1000 æ¡è¾“å‡ºä¸€æ¬¡
        dataset_name=config['name'],
        max_samples=max_samples
    )
    
    return results


def run_reasoning_validity(dataset_key: str, max_samples: int = None):
    """è¿è¡ŒæŒ‡å®šæ•°æ®é›†çš„ Reasoning Validity è¯„ä¼°"""
    if dataset_key not in DATASETS:
        print(f"æœªçŸ¥æ•°æ®é›†: {dataset_key}")
        return
    
    config = DATASETS[dataset_key]
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©è¾“å‡ºç›®å½•
    if 'lila' in dataset_key:
        output_dir = os.path.join(script_dir, 'results', 'lila')
    elif 'openmath' in dataset_key:
        output_dir = os.path.join(script_dir, 'results', 'openmath')
    else:
        output_dir = os.path.join(script_dir, 'results')
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'reasoning_validity_results.json')
    
    print(f"\n{'='*70}")
    print(f"Reasoning Validity è¯„ä¼°: {config['name']}")
    if max_samples:
        print(f"æ ·æœ¬é™åˆ¶: {max_samples}")
    else:
        print(f"æ¨¡å¼: å…¨é‡")
    print(f"{'='*70}\n")
    
    # åˆ›å»º loader
    loader = config['loader_class'](config['data_path'], **config['loader_kwargs'])
    
    # å¦‚æœæœ‰æ ·æœ¬é™åˆ¶ï¼Œä½¿ç”¨ islice
    if max_samples:
        data_iter = itertools.islice(loader.iterate(), max_samples)
    else:
        data_iter = loader.iterate()
    
    # è¿è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨å¹¶è¡Œç‰ˆæœ¬ï¼‰
    results = compute_reasoning_validity_parallel(
        data_iterator=data_iter,
        code_extractor=config['code_extractor'](),
        answer_extractor=config['answer_extractor'](),
        comparator_class=config['comparator'],  # ä½¿ç”¨æ•°æ®é›†ç‰¹å®šçš„æ¯”è¾ƒå™¨
        output_file=output_file,
        progress_interval=100,
        dataset_name=config['name'],
        max_samples=max_samples,
        max_workers=32,  # å¹¶è¡Œè¿›ç¨‹æ•°
    )
    
    return results


def run_format_check(dataset_key: str, max_samples: int = None, parallel: bool = True, num_workers: int = None):
    """è¿è¡ŒæŒ‡å®šæ•°æ®é›†çš„ Format Check è¯„ä¼°"""
    if dataset_key not in DATASETS:
        print(f"æœªçŸ¥æ•°æ®é›†: {dataset_key}")
        return
    
    config = DATASETS[dataset_key]
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©è¾“å‡ºç›®å½•
    if 'lila' in dataset_key:
        output_dir = os.path.join(script_dir, 'results', 'lila')
    elif 'openmath' in dataset_key:
        output_dir = os.path.join(script_dir, 'results', 'openmath')
    else:
        output_dir = os.path.join(script_dir, 'results')
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'format_check_results.json')
    
    print(f"\n{'='*70}")
    print(f"Format Check è¯„ä¼°: {config['name']}")
    if max_samples:
        print(f"æ ·æœ¬é™åˆ¶: {max_samples}")
    else:
        print(f"æ¨¡å¼: å…¨é‡")
    print(f"{'='*70}\n")
    
    # åˆ›å»º loader
    loader = config['loader_class'](config['data_path'], **config['loader_kwargs'])
    
    # å¦‚æœæœ‰æ ·æœ¬é™åˆ¶ï¼Œä½¿ç”¨ islice
    if max_samples:
        data_iter = itertools.islice(loader.iterate(), max_samples)
    else:
        data_iter = loader.iterate()
    
    # è¿è¡Œè¯„ä¼°
    if parallel:
        results = compute_format_check_parallel(
            data_iterator=data_iter,
            format_checker_class=config['format_checker'],
            dataset_name=config['name'],
            output_file=output_file,
            max_samples=max_samples,
            progress_interval=1000,
            max_workers=num_workers,
        )
    else:
        checker = config['format_checker']()
        results = compute_format_check(
            data_iterator=data_iter,
            format_checker=checker,
            dataset_name=config['name'],
            output_file=output_file,
            max_samples=max_samples,
            progress_interval=1000,
        )
    
    return results


def run_diversity(dataset_key: str, max_samples: int = None, method: str = None, 
                  sample_size: int = None, embedding_model: str = None,
                  embedding_batch_size: int = None, vendi_batch_size: int = None, num_gpus: int = None):
    """è¿è¡ŒæŒ‡å®šæ•°æ®é›†çš„ Diversity å¤šæ ·æ€§è¯„ä¼°
    
    Args:
        dataset_key: æ•°æ®é›†æ ‡è¯†
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        method: å¤šæ ·æ€§è®¡ç®—æ–¹æ³• ('knn' æˆ– 'vendi')ï¼ŒNone è¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼
        sample_size: é‡‡æ ·å¤§å°ï¼ŒNone è¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼
        embedding_model: Embedding æ¨¡å‹åç§°ï¼ŒNone è¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼
        embedding_batch_size: Embedding ç”Ÿæˆæ—¶çš„ batch å¤§å°
        vendi_batch_size: Vendi Score åˆ† batch è®¡ç®—çš„å¤§å°
        num_gpus: Vendi Score å¤š GPU å¹¶è¡Œè®¡ç®—æ—¶ä½¿ç”¨çš„ GPU æ•°é‡
    """
    if dataset_key not in DATASETS:
        print(f"æœªçŸ¥æ•°æ®é›†: {dataset_key}")
        return
    
    config = DATASETS[dataset_key]
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©è¾“å‡ºç›®å½•
    if 'lila' in dataset_key:
        output_dir = os.path.join(script_dir, 'results', 'lila')
    elif 'openmath' in dataset_key:
        output_dir = os.path.join(script_dir, 'results', 'openmath')
    else:
        output_dir = os.path.join(script_dir, 'results')
    os.makedirs(output_dir, exist_ok=True)
    
    # ä½¿ç”¨å‚æ•°æˆ–é…ç½®çš„é»˜è®¤å€¼
    diversity_method = method or config.get('diversity_method', 'knn')
    diversity_sample_size = sample_size if sample_size is not None else config.get('diversity_sample_size')
    emb_model = embedding_model or config.get('embedding_model', 'all-MiniLM-L6-v2')
    
    # ç”ŸæˆåŒ…å«æ¨¡å‹åçš„ embedding ç¼“å­˜è·¯å¾„
    model_short_name = emb_model.split('/')[-1] if '/' in emb_model else emb_model
    embedding_cache = os.path.join(script_dir, f'embeddings/{dataset_key}_question_{model_short_name}.npy')
    
    # ç”ŸæˆåŒ…å«æ¨¡å‹åçš„è¾“å‡ºæ–‡ä»¶å
    # å¤„ç†æ¨¡å‹åä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ Qwen/Qwen3-Embedding-8B -> Qwen3-Embedding-8Bï¼‰
    model_short_name = emb_model.split('/')[-1] if '/' in emb_model else emb_model
    output_file = os.path.join(output_dir, f'diversity_{diversity_method}_{model_short_name}_results.json')
    
    print(f"\n{'='*70}")
    print(f"Diversity è¯„ä¼°: {config['name']}")
    print(f"æ–¹æ³•: {diversity_method}")
    print(f"Embedding æ¨¡å‹: {emb_model}")
    if max_samples:
        print(f"æ ·æœ¬é™åˆ¶: {max_samples}")
    else:
        print(f"æ¨¡å¼: å…¨é‡")
    if diversity_sample_size:
        print(f"é‡‡æ ·å¤§å°: {diversity_sample_size}")
    print(f"{'='*70}\n")
    
    # åˆ›å»º loader
    loader = config['loader_class'](config['data_path'], **config['loader_kwargs'])
    
    # å¦‚æœæœ‰æ ·æœ¬é™åˆ¶ï¼Œä½¿ç”¨ islice
    if max_samples:
        data_iter = itertools.islice(loader.iterate(), max_samples)
    else:
        data_iter = loader.iterate()
    
    # è¿è¡Œè¯„ä¼°
    results = compute_diversity(
        data_iterator=data_iter,
        dataset_name=config['name'],
        method=diversity_method,
        field='question',
        embedding_model=emb_model,
        embedding_cache_path=embedding_cache,
        sample_size=diversity_sample_size,
        output_file=output_file,
        max_samples=max_samples,
        embedding_batch_size=embedding_batch_size,
        vendi_batch_size=vendi_batch_size,
        num_gpus=num_gpus,
    )
    
    return results


def main():
    parser = argparse.ArgumentParser(description='æ•°å­¦æ•°æ®é›† Validity + Faithfulness + Reasoning Validity + Format Check + Diversity éªŒè¯')
    parser.add_argument('--dataset', '-d', type=str, default='all',
                        choices=['all'] + list(DATASETS.keys()),
                        help='è¦éªŒè¯çš„æ•°æ®é›† (é»˜è®¤: all)')
    parser.add_argument('--metric', '-m', type=str, default='validity',
                        choices=['validity', 'faithfulness', 'reasoning_validity', 'format_check', 'diversity', 'all'],
                        help='è¯„ä¼°æŒ‡æ ‡ (é»˜è®¤: validity)')
    parser.add_argument('--diversity-method', type=str, default=None,
                        choices=['knn', 'vendi'],
                        help='å¤šæ ·æ€§è®¡ç®—æ–¹æ³• (é»˜è®¤: ä½¿ç”¨é…ç½®)')
    parser.add_argument('--diversity-sample-size', type=int, default=None,
                        help='å¤šæ ·æ€§è®¡ç®—é‡‡æ ·å¤§å° (é»˜è®¤: ä½¿ç”¨é…ç½®)')
    parser.add_argument('--embedding-model', type=str, default=None,
                        help='Embedding æ¨¡å‹: all-MiniLM-L6-v2, all-mpnet-base-v2, Qwen/Qwen3-Embedding-8B')
    parser.add_argument('--embedding-batch-size', type=int, default=None,
                        help='Embedding ç”Ÿæˆæ—¶çš„ batch å¤§å° (é»˜è®¤: 64ï¼Œå¤§æ¨¡å‹å¦‚ 8B å»ºè®®ç”¨ 4-8)')
    parser.add_argument('--vendi-batch-size', type=int, default=None,
                        help='Vendi Score åˆ† batch è®¡ç®—çš„ batch å¤§å°ï¼Œç”¨äºèŠ‚çœæ˜¾å­˜ (é»˜è®¤: None è¡¨ç¤ºä¸åˆ† batchï¼Œå»ºè®®å€¼: 10000-15000)')
    parser.add_argument('--num-gpus', type=int, default=None,
                        help='Vendi Score å¤š GPU å¹¶è¡Œè®¡ç®—æ—¶ä½¿ç”¨çš„ GPU æ•°é‡ (é»˜è®¤: None è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹)')
    parser.add_argument('--max-samples', type=int, default=None,
                        help='è¯„ä¼°çš„æ ·æœ¬æ•° (é»˜è®¤: None è¡¨ç¤ºå…¨é‡)')
    parser.add_argument('--parallel', '-p', action='store_true',
                        help='ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡ŒåŠ é€Ÿ (é»˜è®¤: False)')
    parser.add_argument('--workers', '-w', type=int, default=None,
                        help='å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°)')
    args = parser.parse_args()
    
    datasets_to_run = list(DATASETS.keys()) if args.dataset == 'all' else [args.dataset]
    
    for key in datasets_to_run:
        if args.metric in ['validity', 'all']:
            run_dataset(key, parallel=args.parallel, num_workers=args.workers)
        
        if args.metric in ['faithfulness', 'all']:
            run_faithfulness(key, max_samples=args.max_samples)
        
        if args.metric in ['reasoning_validity', 'all']:
            run_reasoning_validity(key, max_samples=args.max_samples)
        
        if args.metric in ['format_check', 'all']:
            run_format_check(key, max_samples=args.max_samples, parallel=args.parallel, num_workers=args.workers)
        
        if args.metric in ['diversity', 'all']:
            run_diversity(key, max_samples=args.max_samples, method=args.diversity_method, 
                         sample_size=args.diversity_sample_size, embedding_model=args.embedding_model,
                         embedding_batch_size=args.embedding_batch_size,
                         vendi_batch_size=args.vendi_batch_size,
                         num_gpus=args.num_gpus)


if __name__ == '__main__':
    main()
