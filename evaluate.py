#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Training Data Evaluation - ç»Ÿä¸€è¯„ä¼°å…¥å£

ä¸€ä¸ªç»Ÿä¸€çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œå¯ä»¥è¯„ä¼°ä»»æ„æ¨¡æ€çš„è®­ç»ƒæ•°æ®è´¨é‡ã€‚

æ”¯æŒçš„æ¨¡æ€:
- api: API Agent (ToolBench, xLAM)
- gui: GUI Agent (Mind2Web, WebShop, WebLINX)
- math: Math/Symbolic (LILA, OpenMath)
- image: Image-Text
- video: Video-Text

Usage:
    # æŸ¥çœ‹å¸®åŠ©
    python evaluate.py --help
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨é€‰é¡¹
    python evaluate.py --list
    
    # API Agent è¯„æµ‹
    python evaluate.py api toolbench format
    python evaluate.py api toolbench executability
    
    # GUI Agent è¯„æµ‹
    python evaluate.py gui mind2web format
    python evaluate.py gui mind2web static
    python evaluate.py gui weblinx static --max-samples 100
    
    # Math è¯„æµ‹
    python evaluate.py math lila format
    python evaluate.py math lila validity
    
    # é€šç”¨å‚æ•°
    python evaluate.py <modality> <dataset> <metric> [--max-samples N] [--parallel]
"""

import sys
import os
import argparse
import importlib

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)


# =============================================================================
# æ¨¡æ€é…ç½®
# =============================================================================

MODALITIES = {
    'api': {
        'name': 'API Agent',
        'module': 'modalities.Agent_Data.api_agent_eval.scripts.run_full_test',
        'datasets': ['toolbench', 'xlam'],
        'metrics': ['format_check', 'executability', 'dynamic_executability', 'diversity'],
    },
    'gui': {
        'name': 'GUI Agent',
        'module': 'modalities.Agent_Data.text_gui_agent_eval.scripts.run_full_test',
        'datasets': ['mind2web', 'webshop', 'weblinx'],
        'metrics': ['format_check', 'static_executability', 'dynamic_executability', 'html_retention'],
    },
    'math': {
        'name': 'Math/Symbolic',
        'module': 'modalities.Symbolic_and_Logical_Data.math_eval.scripts.run_full_test',
        'datasets': ['lila', 'openmathinstruct'],
        'metrics': ['format_check', 'validity', 'faithfulness', 'reasoning_validity', 'diversity'],
    },
    'image': {
        'name': 'Image-Text',
        'module': 'modalities.Vision_Language_Data.image_text_eval.scripts.run_full_test',
        'datasets': ['coco'],
        'metrics': ['format_check', 'well_formed_rate', 'prompt_fidelity'],
    },
    'video': {
        'name': 'Video-Text',
        'module': 'modalities.Vision_Language_Data.video_text_eval.scripts.run_full_test',
        'datasets': [],
        'metrics': ['holistic_fidelity', 'semantic_diversity', 'safety_bench'],
    },
}


# =============================================================================
# è¾…åŠ©å‡½æ•°
# =============================================================================

def print_banner():
    """æ‰“å°æ¨ªå¹…"""
    print("=" * 70)
    print("  LLM Training Data Evaluation Framework")
    print("  ç»Ÿä¸€çš„è®­ç»ƒæ•°æ®è´¨é‡è¯„æµ‹å·¥å…·")
    print("=" * 70)


def list_all():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨é€‰é¡¹"""
    print_banner()
    print()
    
    for key, config in MODALITIES.items():
        print(f"ğŸ“ {key}: {config['name']}")
        print(f"   Datasets: {', '.join(config['datasets']) if config['datasets'] else '(none)'}")
        print(f"   Metrics:  {', '.join(config['metrics'])}")
        print()
    
    print("-" * 70)
    print("Usage: python evaluate.py <modality> <dataset> <metric> [options]")
    print()
    print("Examples:")
    print("  python evaluate.py api toolbench format_check")
    print("  python evaluate.py gui mind2web static --max-samples 100")
    print("  python evaluate.py math lila validity --parallel")


def run_evaluation(modality: str, dataset: str, metric: str, args, extra_args=None):
    """è¿è¡Œè¯„æµ‹
    
    Args:
        modality: æ¨¡æ€åç§°
        dataset: æ•°æ®é›†åç§°
        metric: æŒ‡æ ‡åç§°
        args: å·²è§£æçš„é€šç”¨å‚æ•°
        extra_args: æœªè§£æçš„é¢å¤–å‚æ•°ï¼Œé€ä¼ ç»™å­è„šæœ¬
    """
    extra_args = extra_args or []
    if modality not in MODALITIES:
        print(f"âŒ Unknown modality: {modality}")
        print(f"   Available: {', '.join(MODALITIES.keys())}")
        sys.exit(1)
    
    config = MODALITIES[modality]
    
    print_banner()
    print()
    print(f"  Modality: {config['name']} ({modality})")
    print(f"  Dataset:  {dataset}")
    print(f"  Metric:   {metric}")
    if args.max_samples:
        print(f"  Samples:  {args.max_samples}")
    if extra_args:
        print(f"  Extra:    {' '.join(extra_args)}")
    print()
    print("-" * 70)
    print()
    
    # æ„å»ºå‘½ä»¤è¡Œå‚æ•°
    cmd_args = [
        '--dataset', dataset,
        '--metric', metric,
    ]
    
    if args.max_samples:
        cmd_args.extend(['--max-samples', str(args.max_samples)])
    if args.parallel:
        cmd_args.append('--parallel')
    if args.workers:
        cmd_args.extend(['--workers', str(args.workers)])
    if args.show:
        cmd_args.append('--show')
    
    # æ·»åŠ é€ä¼ çš„é¢å¤–å‚æ•°
    cmd_args.extend(extra_args)
    
    # åŠ¨æ€å¯¼å…¥å¹¶æ‰§è¡Œ
    try:
        # åˆ‡æ¢åˆ°å¯¹åº”æ¨¡å—ç›®å½•
        module_parts = config['module'].rsplit('.', 1)
        module_dir = os.path.join(PROJECT_ROOT, module_parts[0].replace('.', os.sep))
        
        # ä¿å­˜åŸå§‹ argv å¹¶æ›¿æ¢
        original_argv = sys.argv
        sys.argv = ['run_full_test.py'] + cmd_args
        
        # å¯¼å…¥æ¨¡å—
        module = importlib.import_module(config['module'])
        
        # è°ƒç”¨æ¨¡å—çš„ main() å‡½æ•°
        if hasattr(module, 'main'):
            module.main()
        else:
            print(f"âš ï¸ Module {config['module']} does not have a main() function")
        
        # æ¢å¤ argv
        sys.argv = original_argv
        
    except ImportError as e:
        print(f"âŒ Failed to import module: {config['module']}")
        print(f"   Error: {e}")
        print()
        print("   Try running directly:")
        print(f"   cd {config['module'].rsplit('.', 2)[0].replace('.', '/')}")
        print(f"   python scripts/run_full_test.py --dataset {dataset} --metric {metric}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        raise


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='LLM Training Data Evaluation - ç»Ÿä¸€è¯„æµ‹å…¥å£',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python evaluate.py --list                           # åˆ—å‡ºæ‰€æœ‰é€‰é¡¹
    python evaluate.py api toolbench format_check       # API Agent æ ¼å¼æ£€æŸ¥
    python evaluate.py gui mind2web static              # GUI Agent é™æ€æ£€æŸ¥
    python evaluate.py math lila validity               # Math ä»£ç æ‰§è¡ŒéªŒè¯
    python evaluate.py gui weblinx static --max-samples 100 --show
        """
    )
    
    parser.add_argument('--list', '-l', action='store_true',
                        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡æ€ã€æ•°æ®é›†å’ŒæŒ‡æ ‡')
    parser.add_argument('modality', nargs='?', type=str,
                        help='æ¨¡æ€: api, gui, math, image, video')
    parser.add_argument('dataset', nargs='?', type=str,
                        help='æ•°æ®é›†åç§°')
    parser.add_argument('metric', nargs='?', type=str,
                        help='è¯„æµ‹æŒ‡æ ‡')
    parser.add_argument('--max-samples', type=int, default=None,
                        help='æœ€å¤§æ ·æœ¬æ•°')
    parser.add_argument('--parallel', '-p', action='store_true',
                        help='ä½¿ç”¨å¹¶è¡Œæ¨¡å¼')
    parser.add_argument('--workers', '-w', type=int, default=None,
                        help='å¹¶è¡Œè¿›ç¨‹æ•°')
    parser.add_argument('--show', action='store_true',
                        help='æ˜¾ç¤ºæµè§ˆå™¨ï¼ˆGUI Agentï¼‰')
    
    # ä½¿ç”¨ parse_known_args æ•è·æœªçŸ¥å‚æ•°ï¼Œé€ä¼ ç»™å­è„šæœ¬
    args, extra_args = parser.parse_known_args()
    
    # åˆ—å‡ºæ‰€æœ‰é€‰é¡¹
    if args.list or (not args.modality):
        list_all()
        return
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.dataset:
        print("âŒ Missing dataset. Usage: python evaluate.py <modality> <dataset> <metric>")
        sys.exit(1)
    
    if not args.metric:
        print("âŒ Missing metric. Usage: python evaluate.py <modality> <dataset> <metric>")
        sys.exit(1)
    
    # è¿è¡Œè¯„æµ‹ï¼ˆextra_args é€ä¼ ç»™å­è„šæœ¬ï¼‰
    run_evaluation(args.modality, args.dataset, args.metric, args, extra_args)


if __name__ == '__main__':
    main()
